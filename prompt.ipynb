{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae4f9248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_gene import MoonCalc ,generate_pdf\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b57893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_df(date,month, year, path=\"D:/code/Data_2035\", dst='D:/Output'):\n",
    "    \n",
    "    date_obj = datetime.strptime(date, \"%d-%m-%Y\")    \n",
    "    # Convert to YYYY-MM-DD format\n",
    "    converted_date = date_obj.strftime(\"%Y-%m-%d\")\n",
    "    Moon = MoonCalc(path,converted_date,month,year +\" AH\",dst)\n",
    "    return Moon.calculate()\n",
    "final_df = final_df(\"30-03-2025\",\"SHAWWAL\",\"1446\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bce3941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_df\n",
    "# Split 'STATION(Sunset)' into 'Station' and 'SunsetTime'\n",
    "df[['Station', 'SunsetTime']] = df['STATION(Sunset)'].str.extract(r'^(.*?)\\s*\\((.*?)\\)$')\n",
    "\n",
    "# Optional: drop the original column if no longer needed\n",
    "df.drop(columns=['STATION(Sunset)'], inplace=True)\n",
    "\n",
    "# Reorder columns if desired\n",
    "df = df[['Station', 'SunsetTime', 'LAG TIME(Minutes)', 'MOON ALTITUDE(Degrees)',\n",
    "         'SUN_AZIMUTH(Degrees)', 'DAZ(Degrees)', 'ELONGATION(Degrees)', \n",
    "         'ILLUMINATION(%)', 'CRITERION']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c61c119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/13 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "row = df\n",
    "row.columns = [\n",
    "    'Station', 'SunsetTime', 'LagTime', 'MoonAltitude', 'SunAzimuth',\n",
    "    'DAZ', 'Elongation', 'Illumination', 'Criterion'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# ✅ Function to format the prompt for each row\n",
    "def make_prompt(row):\n",
    "    return (\n",
    "        f\"On {row.name}, at station {row['Station']}, \"\n",
    "        f\"with a lag time of {row['LagTime']} minutes, \"\n",
    "        f\"moon altitude {row['MoonAltitude']}°, \"\n",
    "        f\"sun azimuth {row['SunAzimuth']}°, \"\n",
    "        f\"DAZ {row['DAZ']}°, \"\n",
    "        f\"elongation {row['Elongation']}°, \"\n",
    "        f\"illumination {row['Illumination']}%, \"\n",
    "        f\"and visibility criterion '{row['Criterion']}', \"\n",
    "        f\"explain whether the moon is likely to be visible and why.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ✅ Function to query LLaMA 3.2 via Ollama\n",
    "def query_ollama(prompt, model=\"llama3.2\"):\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\"model\": model, \"prompt\": prompt, \"stream\": False}\n",
    "    )\n",
    "    return response.json()[\"response\"].strip()\n",
    "\n",
    "tqdm.pandas()  # This enables tqdm integration with pandas\n",
    "\n",
    "df['llama_response'] = df.progress_apply(lambda row: query_ollama(make_prompt(row)), axis=1)\n",
    "print(df[['Station', 'llama_response']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fb248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "# df = pd.DataFrame({\n",
    "#     'name': ['Alice', 'Bob'],\n",
    "#     'product': ['laptop', 'smartphone'],\n",
    "#     'issue': ['screen not working', 'battery draining fast']\n",
    "# })\n",
    "\n",
    "df = final_df\n",
    "# Generate a response per row\n",
    "def generate_response(row):\n",
    "    prompt = (\n",
    "        f\"Customer {row['name']} reported an issue with their {row['product']}: \"\n",
    "        f\"'{row['issue']}'. Write a helpful and professional support message.\"\n",
    "    )\n",
    "    response = requests.post(\n",
    "    'http://localhost:11434/api/generate',\n",
    "    json={\n",
    "        \"model\": \"llama3.2\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    )\n",
    "    return response.json()\n",
    "\n",
    "# Apply model to DataFrame\n",
    "df['llama_response'] = df.apply(generate_response, axis=1)\n",
    "\n",
    "# Show results\n",
    "print(df[['name', 'llama_response']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1a7e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "dfd = df\n",
    "\n",
    "# Unpack the dictionary column\n",
    "df_expanded = pd.json_normalize(dfd[\"llama_response\"])\n",
    "\n",
    "# Concatenate the unpacked data with the original DataFrame\n",
    "df_new = pd.concat([dfd, df_expanded], axis=1).drop('llama_response', axis=1)\n",
    "\n",
    "# Display the new DataFrame\n",
    "print(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1245936",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974fe843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Load the model (use the exact filename and path)\n",
    "llm = Llama(model_path=\"llama-3.2-1B.Q4_K_M.gguf\", n_ctx=2048)\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'name': ['Alice', 'Bob'],\n",
    "    'product': ['laptop', 'smartphone'],\n",
    "    'issue': ['screen not working', 'battery draining fast']\n",
    "})\n",
    "\n",
    "# Generate a response per row\n",
    "def generate_response(row):\n",
    "    prompt = (\n",
    "        f\"Customer {row['name']} reported an issue with their {row['product']}: \"\n",
    "        f\"'{row['issue']}'. Write a helpful and professional support message.\"\n",
    "    )\n",
    "    output = llm(prompt, max_tokens=200)\n",
    "    return output[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "# Apply model to DataFrame\n",
    "df['llama_response'] = df.apply(generate_response, axis=1)\n",
    "\n",
    "# Show results\n",
    "print(df[['name', 'llama_response']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6fdd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_pdf(\"30-03-2025\", \"SHAWWAL\",\"1446\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
